{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysus.ftp.databases.sia import SIA\n",
    "from pysus.ftp.databases.sih import SIH\n",
    "from pysus.ftp.databases.cnes import CNES\n",
    "from pysus.ftp.databases.sim import SIM\n",
    "from pysus.ftp.databases.sinasc import SINASC\n",
    "from pysus.ftp.databases.sinan import SINAN\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "sia = SIA().load()\n",
    "sih = SIH().load()\n",
    "cnes = CNES().load()\n",
    "sim = SIM().load()\n",
    "sinasc = SINASC().load()\n",
    "sinan = SINAN().load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tratamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alter_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alterar Tabelas com erro de Date\n",
    "import psycopg2\n",
    "\n",
    "def optimize_alter_table():\n",
    "    # Configuração da conexão com o banco de dados\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"datasus\",\n",
    "        user=\"webadmin\",\n",
    "        password=\"h532947h5g932h\",\n",
    "        host=\"10.100.60.19\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    try:\n",
    "        # Script SQL otimizado para executar alterações\n",
    "        sql_script = \"\"\"\n",
    "        DO $$\n",
    "        BEGIN\n",
    "            -- Alterações na tabela sia_apac_medicamentos\n",
    "            ALTER TABLE sia_apac_medicamentos ALTER COLUMN ap_dtaut TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_medicamentos ALTER COLUMN ap_dtfim TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_medicamentos ALTER COLUMN ap_dtinic TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_medicamentos ALTER COLUMN ap_dtocor TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_medicamentos ALTER COLUMN ap_dtsolic TYPE DATE USING ap_dtfim::DATE;\n",
    "\n",
    "            -- Alterações na tabela sia_apac_tratamento_dialitico\n",
    "            ALTER TABLE sia_apac_tratamento_dialitico ALTER COLUMN ap_dtaut TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_tratamento_dialitico ALTER COLUMN ap_dtfim TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_tratamento_dialitico ALTER COLUMN ap_dtinic TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_tratamento_dialitico ALTER COLUMN ap_dtocor TYPE DATE USING ap_dtfim::DATE;\n",
    "            ALTER TABLE sia_apac_tratamento_dialitico ALTER COLUMN ap_dtsolic TYPE DATE USING ap_dtfim::DATE;\n",
    "\n",
    "            -- Alterações na tabela sia_apac_laudos_diversos\n",
    "            ALTER TABLE sia_apac_laudos_diversos ALTER COLUMN ap_dtsolic TYPE DATE USING ap_dtsolic::DATE;\n",
    "            ALTER TABLE sia_apac_laudos_diversos ALTER COLUMN ap_dtaut TYPE DATE USING ap_dtaut::DATE;\n",
    "\n",
    "            -- Alterações na tabela sia_boletim_producao_ambulatorial_individualizado\n",
    "            ALTER TABLE sia_boletim_producao_ambulatorial_individualizado ALTER COLUMN dtnasc TYPE DATE USING ap_dtaut::DATE;\n",
    "        END $$;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Executar o script SQL\n",
    "        cursor.execute(sql_script)\n",
    "        conn.commit()\n",
    "        print(\"Alterações aplicadas com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        print(\"Erro ao aplicar alterações:\", e)\n",
    "\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimize_alter_table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate_partition_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de tabelas e suas sequências associadas\n",
    "particoes = [\n",
    "    \"sih_servicos_profissionais_temp_part_AC\",\n",
    "    \"sih_servicos_profissionais_temp_part_AL\",\n",
    "    \"sih_servicos_profissionais_temp_part_AM\",\n",
    "    \"sih_servicos_profissionais_temp_part_AP\",\n",
    "    \"sih_servicos_profissionais_temp_part_BA\",\n",
    "    \"sih_servicos_profissionais_temp_part_CE\",\n",
    "    \"sih_servicos_profissionais_temp_part_DF\",\n",
    "    \"sih_servicos_profissionais_temp_part_ES\",\n",
    "    \"sih_servicos_profissionais_temp_part_GO\",\n",
    "    \"sih_servicos_profissionais_temp_part_MA\",\n",
    "    \"sih_servicos_profissionais_temp_part_MG\",\n",
    "    \"sih_servicos_profissionais_temp_part_MS\",\n",
    "    \"sih_servicos_profissionais_temp_part_MT\",\n",
    "    \"sih_servicos_profissionais_temp_part_PA\",\n",
    "    \"sih_servicos_profissionais_temp_part_PB\",\n",
    "    \"sih_servicos_profissionais_temp_part_PE\",\n",
    "    \"sih_servicos_profissionais_temp_part_PI\",\n",
    "    \"sih_servicos_profissionais_temp_part_PR\",\n",
    "    \"sih_servicos_profissionais_temp_part_RJ\",\n",
    "    \"sih_servicos_profissionais_temp_part_RN\",\n",
    "    \"sih_servicos_profissionais_temp_part_RO\",\n",
    "    \"sih_servicos_profissionais_temp_part_RR\",\n",
    "    \"sih_servicos_profissionais_temp_part_RS\",\n",
    "    \"sih_servicos_profissionais_temp_part_SC\",\n",
    "    \"sih_servicos_profissionais_temp_part_SE\",\n",
    "    \"sih_servicos_profissionais_temp_part_SP\",\n",
    "    \"sih_servicos_profissionais_temp_part_TO\"\n",
    "]\n",
    "\n",
    "# Função para gerar SQL dinamicamente\n",
    "def gerar_sql(particoes):\n",
    "    sql_comandos = []\n",
    "    for particao in particoes:\n",
    "        # Gerar o nome da sequência baseado na partição\n",
    "        sequencia = f\"{particao.lower()}_id_seq\"\n",
    "        \n",
    "        # Gerar SQL para associar a sequência à coluna `id`\n",
    "        sql = f\"\"\"\n",
    "        -- Configurar sequência para a partição {particao}\n",
    "        ALTER SEQUENCE {sequencia}\n",
    "        OWNED BY \"{particao}\".id;\n",
    "        \"\"\"\n",
    "        sql_comandos.append(sql.strip())\n",
    "    return sql_comandos\n",
    "\n",
    "# Gerar os comandos SQL\n",
    "comandos_sql = gerar_sql(particoes)\n",
    "\n",
    "# Salvar os comandos em um arquivo ou exibir no console\n",
    "with open(\"configurar_sequencias.sql\", \"w\") as file:\n",
    "    for comando in comandos_sql:\n",
    "        file.write(comando + \"\\n\\n\")\n",
    "\n",
    "# Exibir os comandos gerados\n",
    "for comando in comandos_sql:\n",
    "    print(comando)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate_table_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'sia_tables_columns.json' gerado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import json\n",
    "\n",
    "def fetch_table_columns():\n",
    "    # Configuração da conexão com o banco de dados\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"datasus\",\n",
    "        user=\"webadmin\",\n",
    "        password=\"h532947h5g932h\",\n",
    "        host=\"10.100.60.19\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Consulta para buscar tabelas que começam com 'sih' e suas colunas, incluindo o tamanho para tipos de caracteres\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, column_name, data_type, character_maximum_length\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_schema = 'public'\n",
    "          AND table_name LIKE 'new%'\n",
    "        ORDER BY table_name, column_name;\n",
    "        \"\"\"\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Estrutura do JSON similar a group_info\n",
    "        tables = {}\n",
    "        for table_name, column_name, data_type, char_max_length in results:\n",
    "            if table_name not in tables:\n",
    "                tables[table_name] = {\"tabela\": table_name, \"colunas\": {}}\n",
    "            \n",
    "            # Formatação do tipo com tamanho, se aplicável\n",
    "            tipo_formatado = format_data_type(data_type, char_max_length)\n",
    "            \n",
    "            tables[table_name][\"colunas\"][column_name.lower()] = tipo_formatado\n",
    "\n",
    "        # Salvar em arquivo JSON\n",
    "        with open(\"sih_tables_columns.json\", \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(tables, json_file, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(\"Arquivo 'sia_tables_columns.json' gerado com sucesso.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao executar o script:\", e)\n",
    "    \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "def format_data_type(data_type, char_max_length):\n",
    "    \"\"\"\n",
    "    Formata o tipo de dado incluindo o tamanho para tipos de caracteres.\n",
    "    \"\"\"\n",
    "    data_type_upper = data_type.upper()\n",
    "    if data_type_upper in [\"CHARACTER VARYING\", \"VARCHAR\"]:\n",
    "        if char_max_length:\n",
    "            return f\"VARCHAR({char_max_length})\"\n",
    "        else:\n",
    "            return \"VARCHAR\"\n",
    "    elif data_type_upper == \"CHARACTER\":\n",
    "        if char_max_length:\n",
    "            return f\"CHAR({char_max_length})\"\n",
    "        else:\n",
    "            return \"CHAR\"\n",
    "    elif data_type_upper == \"USER-DEFINED\":\n",
    "        # Trata tipos definidos pelo usuário, se houver\n",
    "        return \"USER_DEFINED\"\n",
    "    else:\n",
    "        # Para outros tipos, retorna apenas o tipo em maiúsculas\n",
    "        return data_type_upper\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_table_columns()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gerar_sql_correcao_sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extrair_tabela_coluna(linha_contexto):\n",
    "    \"\"\"\n",
    "    Extrai o nome da tabela e da coluna a partir de uma linha de contexto.\n",
    "\n",
    "    Exemplo de linha de contexto:\n",
    "    CONTEXT:  COPY sia_apac_laudos_diversos, line 1, column ap_dtaut: \"2009-10-01\"\n",
    "    \"\"\"\n",
    "    regex = r'COPY\\s+(\\w+),\\s+line\\s+\\d+,\\s+column\\s+(\\w+):\\s+\".+\"'\n",
    "    match = re.search(regex, linha_contexto)\n",
    "    if match:\n",
    "        tabela = match.group(1)\n",
    "        coluna = match.group(2)\n",
    "        return tabela, coluna\n",
    "    return None, None\n",
    "\n",
    "def gerar_comandos_sql(pares_tabela_coluna):\n",
    "    \"\"\"\n",
    "    Gera comandos SQL para alterar o tipo de colunas para DATE.\n",
    "\n",
    "    :param pares_tabela_coluna: Set contendo tuplas (tabela, coluna)\n",
    "    :return: Lista de strings com comandos SQL\n",
    "    \"\"\"\n",
    "    comandos = []\n",
    "    for tabela, coluna in pares_tabela_coluna:\n",
    "        comando = f'ALTER TABLE {tabela} ALTER COLUMN {coluna} TYPE DATE USING {coluna}::DATE;'\n",
    "        comandos.append(comando)\n",
    "    return comandos\n",
    "\n",
    "def processar_log(caminho_log):\n",
    "    \"\"\"\n",
    "    Processa o arquivo de log para extrair pares tabela-coluna de erros críticos.\n",
    "\n",
    "    :param caminho_log: Caminho para o arquivo de log\n",
    "    :return: Set contendo tuplas (tabela, coluna)\n",
    "    \"\"\"\n",
    "    pares_tabela_coluna = set()\n",
    "    \n",
    "    # Verifica se o arquivo existe\n",
    "    if not os.path.isfile(caminho_log):\n",
    "        print(f\"Erro: O arquivo {caminho_log} não foi encontrado.\")\n",
    "        return pares_tabela_coluna\n",
    "    \n",
    "    with open(caminho_log, 'r', encoding='utf-8') as arquivo:\n",
    "        linhas = arquivo.readlines()\n",
    "    \n",
    "    for i, linha in enumerate(linhas):\n",
    "        if 'CRITICAL' in linha:\n",
    "            # Procura pela linha CONTEXT na mesma linha ou nas próximas 2 linhas\n",
    "            contexto = None\n",
    "            # Verifica até 2 linhas à frente para encontrar o CONTEXT\n",
    "            for j in range(1, 3):\n",
    "                if i + j < len(linhas):\n",
    "                    linha_proxima = linhas[i + j].strip()\n",
    "                    if linha_proxima.startswith('CONTEXT:'):\n",
    "                        contexto = linha_proxima\n",
    "                        break\n",
    "            if contexto:\n",
    "                tabela, coluna = extrair_tabela_coluna(contexto)\n",
    "                if tabela and coluna:\n",
    "                    pares_tabela_coluna.add((tabela, coluna))\n",
    "    \n",
    "    return pares_tabela_coluna\n",
    "\n",
    "def main():\n",
    "    caminho_log = './logs/upload_sia_2959293.log'  # Substitua pelo caminho correto do seu arquivo de log\n",
    "    pares = processar_log(caminho_log)\n",
    "    \n",
    "    if not pares:\n",
    "        print(\"Nenhum erro crítico relevante encontrado no log.\")\n",
    "        return\n",
    "    \n",
    "    comandos_sql = gerar_comandos_sql(pares)\n",
    "    \n",
    "    # Exibe os comandos gerados\n",
    "    print(\"Comandos SQL para alterar colunas para tipo DATE:\\n\")\n",
    "    for comando in comandos_sql:\n",
    "        print(comando)\n",
    "    \n",
    "    # Opcional: Salva os comandos em um arquivo SQL\n",
    "    caminho_saida = 'alterar_colunas_para_date.sql'\n",
    "    with open(caminho_saida, 'w', encoding='utf-8') as f:\n",
    "        for comando in comandos_sql:\n",
    "            f.write(comando + '\\n')\n",
    "    \n",
    "    print(f\"\\nOs comandos SQL foram salvos no arquivo: {caminho_saida}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gerar_tabelas_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando queries SQL de criação de tabelas a partir de 'grupos_info.json'...\n",
      "Todas as queries foram geradas e salvas em 'criar_tabelas.sql'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalizar_nome(nome):\n",
    "    \"\"\"\n",
    "    Normaliza o nome:\n",
    "    - Converte para minúsculas.\n",
    "    - Remove acentos e caracteres especiais.\n",
    "    - Substitui caracteres não alfanuméricos por sublinhado '_'.\n",
    "    - Remove sublinhados extras no início e no fim.\n",
    "    \n",
    "    Args:\n",
    "        nome (str): Nome a ser normalizado.\n",
    "    \n",
    "    Returns:\n",
    "        str: Nome normalizado.\n",
    "    \"\"\"\n",
    "    nome = nome.lower()\n",
    "    nome = unicodedata.normalize('NFKD', nome).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    nome = re.sub(r'\\W+', '_', nome)\n",
    "    nome = nome.strip('_')\n",
    "    return nome\n",
    "\n",
    "def mapear_tipo_postgres(tipo):\n",
    "    \"\"\"\n",
    "    Mapeia o tipo de dado para o tipo correspondente no PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        tipo (str): Tipo de dado original.\n",
    "    \n",
    "    Returns:\n",
    "        str: Tipo de dado mapeado para PostgreSQL.\n",
    "    \"\"\"\n",
    "    # Remover espaços extras e converter para maiúsculas\n",
    "    tipo = tipo.strip().upper()\n",
    "    \n",
    "    # Dicionário de mapeamento básico\n",
    "    mapeamento = {\n",
    "        'SMALLINT': 'SMALLINT',\n",
    "        'INTEGER': 'INTEGER',\n",
    "        'BIGINT': 'BIGINT',\n",
    "        'NUMERIC': 'NUMERIC',\n",
    "        'BOOLEAN': 'BOOLEAN',\n",
    "        'DATE': 'DATE',\n",
    "        'TIMESTAMP': 'TIMESTAMP',\n",
    "        'TEXT': 'TEXT',\n",
    "        'SERIAL': 'SERIAL',\n",
    "        'VARCHAR': 'VARCHAR',\n",
    "        'CHAR': 'CHAR'\n",
    "    }\n",
    "    \n",
    "    # Verificar se o tipo é NUMERIC com precisão\n",
    "    if tipo.startswith('NUMERIC'):\n",
    "        return tipo  # Mantém como está (e.g., NUMERIC(10,2))\n",
    "    \n",
    "    # Verificar se é CHAR(n) ou VARCHAR(n)\n",
    "    match = re.match(r'(CHAR|VARCHAR)\\((\\d+)\\)', tipo)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}({match.group(2)})\"\n",
    "    \n",
    "    # Se o tipo estiver no mapeamento, retorna o mapeamento\n",
    "    if tipo in mapeamento:\n",
    "        return mapeamento[tipo]\n",
    "    else:\n",
    "        # Caso contrário, retorna TEXT como padrão para tipos desconhecidos\n",
    "        print(f\"Aviso: Tipo de dado desconhecido '{tipo}'. Usando 'TEXT' como padrão.\")\n",
    "        return 'TEXT'\n",
    "\n",
    "def gerar_queries_criacao_tabelas(grupos_info, nome_arquivo_sql='criar_tabelas.sql'):\n",
    "    \"\"\"\n",
    "    Gera queries SQL para criar tabelas com base no dicionário GRUPOS_INFO.\n",
    "    \n",
    "    Args:\n",
    "        grupos_info (dict): Dicionário contendo informações dos grupos e suas tabelas.\n",
    "        nome_arquivo_sql (str): Nome do arquivo .sql onde as queries serão salvas (opcional).\n",
    "    \n",
    "    Returns:\n",
    "        str: String contendo todas as queries SQL de criação de tabelas.\n",
    "    \"\"\"\n",
    "    queries = \"\"\n",
    "    for grupo, info in grupos_info.items():\n",
    "        tabela = info.get('tabela')\n",
    "        colunas = info.get('colunas', {})\n",
    "        \n",
    "        if not tabela:\n",
    "            print(f\"Aviso: O grupo '{grupo}' não possui um nome de tabela definido.\")\n",
    "            continue\n",
    "        \n",
    "        # Iniciar a query de criação da tabela\n",
    "        query = f\"CREATE TABLE IF NOT EXISTS {tabela} (\\n\"\n",
    "        \n",
    "        # Adicionar a coluna id como PRIMARY KEY com sequência automática\n",
    "        query += \"    id SERIAL PRIMARY KEY,\\n\"\n",
    "        \n",
    "        colunas_definicoes = []\n",
    "        for coluna, tipo in colunas.items():\n",
    "            coluna_norm = normalizar_nome(coluna)\n",
    "            tipo_postgres = mapear_tipo_postgres(tipo)\n",
    "            colunas_definicoes.append(f\"    {coluna_norm} {tipo_postgres}\")\n",
    "        \n",
    "        # Unir todas as definições de colunas com vírgulas\n",
    "        query += \",\\n\".join(colunas_definicoes)\n",
    "        query += \"\\n);\\n\\n\"\n",
    "        \n",
    "        # Adicionar a query ao conjunto total\n",
    "        queries += query\n",
    "    \n",
    "    # Salvar as queries em um arquivo .sql\n",
    "    try:\n",
    "        with open(nome_arquivo_sql, 'w', encoding='utf-8') as f:\n",
    "            f.write(queries)\n",
    "        print(f\"Todas as queries foram geradas e salvas em '{nome_arquivo_sql}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o arquivo '{nome_arquivo_sql}': {e}\")\n",
    "    \n",
    "    return queries\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Função principal que coordena a geração das queries SQL a partir do arquivo grupos_info.json.\n",
    "    \"\"\"\n",
    "    # Caminho para o arquivo grupos_info.json\n",
    "    caminho_grupos_info = \"../grupos_info.json\"\n",
    "    \n",
    "    # Verificar se o arquivo grupos_info.json existe\n",
    "    if not os.path.exists(caminho_grupos_info):\n",
    "        print(f\"Erro: O arquivo '{caminho_grupos_info}' não foi encontrado.\")\n",
    "        return\n",
    "    \n",
    "    # Carregar o conteúdo de grupos_info.json\n",
    "    try:\n",
    "        with open(caminho_grupos_info, 'r', encoding='utf-8') as f:\n",
    "            grupos_info = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Erro ao decodificar JSON no arquivo '{caminho_grupos_info}': {e}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo '{caminho_grupos_info}': {e}\")\n",
    "        return\n",
    "    \n",
    "    # Verificar se GRUPOS_INFO está no formato correto\n",
    "    if not isinstance(grupos_info, dict):\n",
    "        print(f\"Erro: O conteúdo de '{caminho_grupos_info}' não está no formato esperado (dicionário).\")\n",
    "        return\n",
    "    \n",
    "    # Gerar as queries de criação de tabelas\n",
    "    print(\"Gerando queries SQL de criação de tabelas a partir de 'grupos_info.json'...\")\n",
    "    gerar_queries_criacao_tabelas(grupos_info)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
